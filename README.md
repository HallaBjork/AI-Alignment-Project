# When AI Plays Pretend: How Kids Playful Curiosity Can Lead to Hidden Risks (And What Parents Can Do)

Children's "creative misuse" of AI (e.g., "Teach me to hack TikTok!") reveals vulnerabilities in LLMs. This research investigates how their imaginative interactions (role-play, trick questions, etc.) expose alignment failures in LLMs like DeepSeek-V3, GPT-4, and Claude, potentially causing harms like misinformation and manipulation. Prompting experiments aim to identify specific alignment failures (misinformation, manipulation, privacy, bias) and provide actionable insights for parents to promote safer AI for children.
This research project was undertaken as part of the AI Safety Fundamentals course on AI alignment, offered by BlueDot Impact. Please keep in mind that the explanations provided here are intended as preliminary insights and, for the sake of clarity, may not fully capture the complexities inherent in aligning AI systems. The project's repository is available here.

___

![Screenshot](https://raw.githubusercontent.com/HallaBjork/AI-Alignment-Project/main/docs/Screenshot-2025-02-02.png)

___

**Disclaimer:** ****The views and opinions expressed in this project are solely my own and do not reflect those of my employer, colleagues, or any other individuals or organizations. This project is a personal endeavor and is not affiliated with or endorsed by any other entity.****

Please check out the project post: [https://hallabjork.github.io/AI-Alignment-Project/](https://docs.google.com/document/d/1ae-HL1gGZNwrC1Svuv86_acifYYvGq0-5a2uKt6r8zc/edit?tab=t.0)

This is a link to the experiement result: https://docs.google.com/spreadsheets/d/1I8d-OkihyLmMhizLTOnYtXXJ8-UkX1gmYBGla-yPVgI/edit?gid=157145675#gid=157145675

